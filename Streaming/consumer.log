2017-03-27 14:00:34 INFO  FlinkKafkaConsumer:587 - Trying to get topic metadata from broker 10.6.185.142:6667 in try 0/3
2017-03-27 14:00:34 DEBUG SimpleConsumer:52 - Disconnecting from 10.6.185.142:6667
2017-03-27 14:00:34 DEBUG BlockingChannel:52 - Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 8192 (requested -1), connectTimeoutMs = 30000.
2017-03-27 14:00:34 DEBUG SimpleConsumer:52 - Disconnecting from 10.6.185.142:6667
2017-03-27 14:00:34 INFO  FlinkKafkaConsumer:292 - Topic fast-messages has 1 partitions
2017-03-27 14:00:34 DEBUG StreamGraph:176 - Vertex: 1
2017-03-27 14:00:34 DEBUG StreamGraph:176 - Vertex: 2
2017-03-27 14:00:34 DEBUG StreamGraph:176 - Vertex: 3
2017-03-27 14:00:34 DEBUG StreamingJobGraphGenerator:235 - Parallelism set: 1 for 2
2017-03-27 14:00:34 DEBUG StreamingJobGraphGenerator:235 - Parallelism set: 4 for 1
2017-03-27 14:00:34 DEBUG StreamingJobGraphGenerator:312 - CONNECTED: RebalancePartitioner - 1 -> 2
2017-03-27 14:00:34 INFO  ClusterUtil:54 - Running on mini cluster
2017-03-27 14:00:35 INFO  Slf4jLogger:80 - Slf4jLogger started
2017-03-27 14:00:35 INFO  BlobServer:83 - Created BLOB server storage directory C:\Users\20597\AppData\Local\Temp\blobStore-e508ae63-2a7e-4a39-b75d-dd1b1dd48bba
2017-03-27 14:00:35 INFO  BlobServer:122 - Started BLOB server at 0.0.0.0:50850 - max concurrent requests: 50 - max backlog: 1000
2017-03-27 14:00:35 INFO  JobManager:128 - Starting JobManager at akka://flink/user/jobmanager#632585477.
2017-03-27 14:00:35 WARN  InstanceConnectionInfo:103 - No hostname could be resolved for the IP address 127.0.0.1, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.
2017-03-27 14:00:35 INFO  TaskManager:128 - Messages between TaskManager and JobManager have a max timeout of 100000 milliseconds
2017-03-27 14:00:35 INFO  TaskManager:128 - Temporary file directory 'C:\Users\20597\AppData\Local\Temp': total 99 GB, usable 5 GB (5.05% usable)
2017-03-27 14:00:35 INFO  NetworkBufferPool:101 - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2017-03-27 14:00:35 INFO  TaskManager:128 - Using 2492 MB for Flink managed memory.
2017-03-27 14:00:37 INFO  IOManager:97 - I/O manager uses directory C:\Users\20597\AppData\Local\Temp\flink-io-595b6235-65bd-4161-8515-fc614d04f1e6 for spill files.
2017-03-27 14:00:37 INFO  FileCache:88 - User file cache uses directory C:\Users\20597\AppData\Local\Temp\flink-dist-cache-147c1bbc-371c-4d0c-b3bf-ce4aa8b3cbba
2017-03-27 14:00:37 INFO  TaskManager:128 - Starting TaskManager actor at akka://flink/user/taskmanager_1#935127370.
2017-03-27 14:00:37 INFO  TaskManager:128 - TaskManager data connection information: 127.0.0.1 (dataPort=50851)
2017-03-27 14:00:37 INFO  TaskManager:128 - TaskManager has 4 task slot(s).
2017-03-27 14:00:37 INFO  TaskManager:128 - Memory usage stats: [HEAP: 2598/3593/7257 MB, NON HEAP: 22/35/130 MB (used/committed/max)]
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message NotifyWhenRegisteredAtJobManager at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$a].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message NotifyWhenRegisteredAtJobManager in 0 ms from Actor[akka://flink/temp/$a].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message TriggerTaskManagerRegistration(akka://flink/user/jobmanager,500 milliseconds,None,1) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 INFO  TaskManager:128 - Trying to register at JobManager akka://flink/user/jobmanager (attempt 1, timeout: 500 milliseconds)
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message RegisterTaskManager(Actor[akka://flink/user/taskmanager_1#935127370],127.0.0.1 (dataPort=50851),cores=4, physMem=34359738368, heap=7609516032, managed=2613051392,4) at akka://flink/user/jobmanager from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message TriggerTaskManagerRegistration(akka://flink/user/jobmanager,500 milliseconds,None,1) in 8 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 INFO  InstanceManager:161 - Registered TaskManager at 127.0.0.1 (akka://flink/user/taskmanager_1) as 7f747259085967c272ac26bfa10c5a15. Current number of registered hosts is 1.
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message AcknowledgeRegistration(Actor[akka://flink/user/jobmanager#632585477],7f747259085967c272ac26bfa10c5a15,50850) at akka://flink/user/taskmanager_1 from Actor[akka://flink/user/jobmanager#632585477].
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message RegisterTaskManager(Actor[akka://flink/user/taskmanager_1#935127370],127.0.0.1 (dataPort=50851),cores=4, physMem=34359738368, heap=7609516032, managed=2613051392,4) in 6 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:37 INFO  TaskManager:128 - Successful registration at JobManager (akka://flink/user/jobmanager), starting network stack and library cache.
2017-03-27 14:00:37 DEBUG NetworkEnvironment:181 - Starting result partition manager and network connection manager
2017-03-27 14:00:37 DEBUG NetworkEnvironment:196 - Starting network connection manager
2017-03-27 14:00:37 INFO  TaskManager:128 - Determined BLOB server address to be localhost/127.0.0.1:50850. Starting BLOB cache.
2017-03-27 14:00:37 INFO  BlobCache:70 - Created BLOB cache storage directory C:\Users\20597\AppData\Local\Temp\blobStore-c23eae76-f326-422d-bd28-e62920a86287
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message AcknowledgeRegistration(Actor[akka://flink/user/jobmanager#632585477],7f747259085967c272ac26bfa10c5a15,50850) in 17 ms from Actor[akka://flink/user/jobmanager#632585477].
2017-03-27 14:00:37 INFO  JobClient:79 - Sending message to JobManager akka://flink/user/jobmanager to submit job Socket Window WordCount (849a403605bf5d7e8d14df90a927de42) and wait for progress
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message SubmitJob(org.apache.flink.runtime.jobgraph.JobGraph@526e74f5,true) at akka://flink/user/jobmanager from Actor[akka://flink/user/$a#-252077346].
2017-03-27 14:00:37 INFO  JobManager:128 - Received job 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount).
2017-03-27 14:00:37 DEBUG JobManager:86 - Running initialization on master for job 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount).
2017-03-27 14:00:37 DEBUG JobManager:86 - Adding 2 vertices from job graph 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount).
2017-03-27 14:00:37 DEBUG ExecutionGraph:460 - Attaching 2 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
2017-03-27 14:00:37 DEBUG ExecutionGraph:223 - Connecting ExecutionJobVertex 236a260bab4ca6d770bbfcbc2a89cfb8 (Custom Source -> Stream Sink) to 0 predecessors.
2017-03-27 14:00:37 DEBUG ExecutionGraph:223 - Connecting ExecutionJobVertex a63137b602209a3152a8545c80149a16 (Stream Sink) to 1 predecessors.
2017-03-27 14:00:37 DEBUG ExecutionGraph:234 - Connecting input 0 of vertex a63137b602209a3152a8545c80149a16 (Stream Sink) to intermediate result referenced via predecessor 236a260bab4ca6d770bbfcbc2a89cfb8 (Custom Source -> Stream Sink).
2017-03-27 14:00:37 DEBUG JobManager:86 - Successfully created execution graph from job graph 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount).
2017-03-27 14:00:37 INFO  JobClient:114 - Job was successfully submitted to the JobManager
2017-03-27 14:00:37 INFO  JobManager:128 - Scheduling job Socket Window WordCount.
2017-03-27 14:00:37 DEBUG ExecutionGraph:667 - Socket Window WordCount switched from CREATED to RUNNING.
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Job execution switched to status RUNNING.
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb) switched from CREATED to SCHEDULED
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(1/4) switched to SCHEDULED 
2017-03-27 14:00:37 DEBUG Scheduler:156 - Scheduling task {task=Custom Source -> Stream Sink (1/4) - execution #0, sharingUnit=SlotSharingGroup [a63137b602209a3152a8545c80149a16, 236a260bab4ca6d770bbfcbc2a89cfb8], locationConstraint=null}
2017-03-27 14:00:37 DEBUG Scheduler:593 - Unconstrained assignment: Custom Source -> Stream Sink (1/4) --> SimpleSlot (0)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - ALLOCATED/ALIVE
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb) switched from SCHEDULED to DEPLOYING
2017-03-27 14:00:37 INFO  ExecutionGraph:331 - Deploying Custom Source -> Stream Sink (1/4) (attempt #0) to 127.0.0.1
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(1/4) switched to DEPLOYING 
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (2/4) (de70f918d457c7420927bf48e98fa5a1) switched from CREATED to SCHEDULED
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(2/4) switched to SCHEDULED 
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: 867c029d7acfa61e26a52e01daa2acbb, task name: Custom Source -> Stream Sink (0/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: 45df879dca8169d5c388e11d32d1a5ef, partition type: PIPELINED]], input gates: []]) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$b].
2017-03-27 14:00:37 DEBUG Scheduler:156 - Scheduling task {task=Custom Source -> Stream Sink (2/4) - execution #0, sharingUnit=SlotSharingGroup [a63137b602209a3152a8545c80149a16, 236a260bab4ca6d770bbfcbc2a89cfb8], locationConstraint=null}
2017-03-27 14:00:37 DEBUG Scheduler:593 - Unconstrained assignment: Custom Source -> Stream Sink (2/4) --> SimpleSlot (1)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - ALLOCATED/ALIVE
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (2/4) (de70f918d457c7420927bf48e98fa5a1) switched from SCHEDULED to DEPLOYING
2017-03-27 14:00:37 INFO  ExecutionGraph:331 - Deploying Custom Source -> Stream Sink (2/4) (attempt #0) to 127.0.0.1
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(2/4) switched to DEPLOYING 
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf) switched from CREATED to SCHEDULED
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(3/4) switched to SCHEDULED 
2017-03-27 14:00:37 DEBUG Scheduler:156 - Scheduling task {task=Custom Source -> Stream Sink (3/4) - execution #0, sharingUnit=SlotSharingGroup [a63137b602209a3152a8545c80149a16, 236a260bab4ca6d770bbfcbc2a89cfb8], locationConstraint=null}
2017-03-27 14:00:37 DEBUG Scheduler:593 - Unconstrained assignment: Custom Source -> Stream Sink (3/4) --> SimpleSlot (2)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - ALLOCATED/ALIVE
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf) switched from SCHEDULED to DEPLOYING
2017-03-27 14:00:37 INFO  ExecutionGraph:331 - Deploying Custom Source -> Stream Sink (3/4) (attempt #0) to 127.0.0.1
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(3/4) switched to DEPLOYING 
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3) switched from CREATED to SCHEDULED
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(4/4) switched to SCHEDULED 
2017-03-27 14:00:37 DEBUG Scheduler:156 - Scheduling task {task=Custom Source -> Stream Sink (4/4) - execution #0, sharingUnit=SlotSharingGroup [a63137b602209a3152a8545c80149a16, 236a260bab4ca6d770bbfcbc2a89cfb8], locationConstraint=null}
2017-03-27 14:00:37 DEBUG Scheduler:593 - Unconstrained assignment: Custom Source -> Stream Sink (4/4) --> SimpleSlot (3)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - ALLOCATED/ALIVE
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3) switched from SCHEDULED to DEPLOYING
2017-03-27 14:00:37 INFO  ExecutionGraph:331 - Deploying Custom Source -> Stream Sink (4/4) (attempt #0) to 127.0.0.1
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(4/4) switched to DEPLOYING 
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc) switched from CREATED to SCHEDULED
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Stream Sink(1/1) switched to SCHEDULED 
2017-03-27 14:00:37 DEBUG Scheduler:156 - Scheduling task {task=Stream Sink (1/1) - execution #0, sharingUnit=SlotSharingGroup [a63137b602209a3152a8545c80149a16, 236a260bab4ca6d770bbfcbc2a89cfb8], locationConstraint=null}
2017-03-27 14:00:37 DEBUG ResultPartition:166 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb): Initialized ResultPartition c388e11d32d1a5ef@26a52e01daa2acbb [PIPELINED, 1 subpartitions, 1 pending references]
2017-03-27 14:00:37 DEBUG Scheduler:596 - Local assignment: Stream Sink (1/1) --> SimpleSlot (3)(1) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - ALLOCATED/ALIVE
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc) switched from SCHEDULED to DEPLOYING
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Stream Sink(1/1) switched to DEPLOYING 
2017-03-27 14:00:37 INFO  ExecutionGraph:331 - Deploying Stream Sink (1/1) (attempt #0) to 127.0.0.1
2017-03-27 14:00:37 DEBUG InputChannelDeploymentDescriptor:134 - Created [InputChannelDeploymentDescriptor [consumed partition id: c388e11d32d1a5ef@26a52e01daa2acbb, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: e052049d23e06b0b@0927bf48e98fa5a1, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 9bfd78352516b342@56e70c43532d8fcf, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 6a55e831d2538c95@449703fe2f7f31e3, consumed partition location: ResultPartitionLocation [UNKNOWN]]] from edges [ExecutionEdge [org.apache.flink.runtime.executiongraph.IntermediateResultPartition@6f261d52 <=> Stream Sink (1/1)], ExecutionEdge [org.apache.flink.runtime.executiongraph.IntermediateResultPartition@2f4f2b80 <=> Stream Sink (1/1)], ExecutionEdge [org.apache.flink.runtime.executiongraph.IntermediateResultPartition@76b0f8c3 <=> Stream Sink (1/1)], ExecutionEdge [org.apache.flink.runtime.executiongraph.IntermediateResultPartition@5c9c0715 <=> Stream Sink (1/1)]].
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message SubmitJob(org.apache.flink.runtime.jobgraph.JobGraph@526e74f5,true) in 48 ms from Actor[akka://flink/user/$a#-252077346].
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message 03/27/2017 14:00:37	Job execution switched to status RUNNING. at akka://flink/user/jobmanager from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 INFO  TaskManager:128 - Received task Custom Source -> Stream Sink (1/4)
2017-03-27 14:00:37 INFO  Task:446 - Loading JAR files for task Custom Source -> Stream Sink (1/4)
2017-03-27 14:00:37 INFO  JobManager:137 - Status of job 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount) changed to RUNNING.
2017-03-27 14:00:37 DEBUG Task:679 - Register task 867c029d7acfa61e26a52e01daa2acbb at library cache manager took 0 milliseconds
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message 03/27/2017 14:00:37	Job execution switched to status RUNNING. in 1 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 INFO  Task:463 - Registering task at network: Custom Source -> Stream Sink (1/4) [DEPLOYING]
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: 867c029d7acfa61e26a52e01daa2acbb, task name: Custom Source -> Stream Sink (0/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: 45df879dca8169d5c388e11d32d1a5ef, partition type: PIPELINED]], input gates: []]) in 17 ms from Actor[akka://flink/temp/$b].
2017-03-27 14:00:37 DEBUG ResultPartitionManager:61 - Registered ResultPartition c388e11d32d1a5ef@26a52e01daa2acbb [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: de70f918d457c7420927bf48e98fa5a1, task name: Custom Source -> Stream Sink (1/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: 5a4e46cb9c25871ae052049d23e06b0b, partition type: PIPELINED]], input gates: []]) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$c].
2017-03-27 14:00:37 DEBUG ResultPartition:166 - Custom Source -> Stream Sink (2/4) (de70f918d457c7420927bf48e98fa5a1): Initialized ResultPartition e052049d23e06b0b@0927bf48e98fa5a1 [PIPELINED, 1 subpartitions, 1 pending references]
2017-03-27 14:00:37 INFO  TaskManager:128 - Received task Custom Source -> Stream Sink (2/4)
2017-03-27 14:00:37 INFO  Task:446 - Loading JAR files for task Custom Source -> Stream Sink (2/4)
2017-03-27 14:00:37 INFO  StreamTask:131 - State backend for state checkpoints is set to jobmanager.
2017-03-27 14:00:37 DEBUG Task:679 - Register task de70f918d457c7420927bf48e98fa5a1 at library cache manager took 0 milliseconds
2017-03-27 14:00:37 INFO  Task:463 - Registering task at network: Custom Source -> Stream Sink (2/4) [DEPLOYING]
2017-03-27 14:00:37 DEBUG ResultPartitionManager:61 - Registered ResultPartition e052049d23e06b0b@0927bf48e98fa5a1 [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:37 INFO  StreamTask:131 - State backend for state checkpoints is set to jobmanager.
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: de70f918d457c7420927bf48e98fa5a1, task name: Custom Source -> Stream Sink (1/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: 5a4e46cb9c25871ae052049d23e06b0b, partition type: PIPELINED]], input gates: []]) in 1 ms from Actor[akka://flink/temp/$c].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: b5231155e7318a7556e70c43532d8fcf, task name: Custom Source -> Stream Sink (2/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: c17e3ae68cb229c49bfd78352516b342, partition type: PIPELINED]], input gates: []]) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$d].
2017-03-27 14:00:37 DEBUG ResultPartition:166 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf): Initialized ResultPartition 9bfd78352516b342@56e70c43532d8fcf [PIPELINED, 1 subpartitions, 1 pending references]
2017-03-27 14:00:37 INFO  TaskManager:128 - Received task Custom Source -> Stream Sink (3/4)
2017-03-27 14:00:37 INFO  Task:446 - Loading JAR files for task Custom Source -> Stream Sink (3/4)
2017-03-27 14:00:37 DEBUG Task:679 - Register task b5231155e7318a7556e70c43532d8fcf at library cache manager took 0 milliseconds
2017-03-27 14:00:37 INFO  Task:463 - Registering task at network: Custom Source -> Stream Sink (3/4) [DEPLOYING]
2017-03-27 14:00:37 DEBUG ResultPartitionManager:61 - Registered ResultPartition 9bfd78352516b342@56e70c43532d8fcf [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:37 INFO  StreamTask:131 - State backend for state checkpoints is set to jobmanager.
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: b5231155e7318a7556e70c43532d8fcf, task name: Custom Source -> Stream Sink (2/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: c17e3ae68cb229c49bfd78352516b342, partition type: PIPELINED]], input gates: []]) in 1 ms from Actor[akka://flink/temp/$d].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: ff3a049e2641d373449703fe2f7f31e3, task name: Custom Source -> Stream Sink (3/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: 07eb7cc6bd3a79056a55e831d2538c95, partition type: PIPELINED]], input gates: []]) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$e].
2017-03-27 14:00:37 DEBUG ResultPartition:166 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3): Initialized ResultPartition 6a55e831d2538c95@449703fe2f7f31e3 [PIPELINED, 1 subpartitions, 1 pending references]
2017-03-27 14:00:37 INFO  TaskManager:128 - Received task Custom Source -> Stream Sink (4/4)
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: 236a260bab4ca6d770bbfcbc2a89cfb8, execution id: ff3a049e2641d373449703fe2f7f31e3, task name: Custom Source -> Stream Sink (3/4), invokable: org.apache.flink.streaming.runtime.tasks.SourceStreamTask, produced partitions: [ResultPartitionDeploymentDescriptor [result id: eb1f4e070d112ff527b2d0891784003b, partition id: 07eb7cc6bd3a79056a55e831d2538c95, partition type: PIPELINED]], input gates: []]) in 1 ms from Actor[akka://flink/temp/$e].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: a63137b602209a3152a8545c80149a16, execution id: cf45bb49724013c386aab08613f225dc, task name: Stream Sink (0/1), invokable: org.apache.flink.streaming.runtime.tasks.OneInputStreamTask, produced partitions: [], input gates: [InputGateDeploymentDescriptor [result id: 27b2d0891784003b, consumed subpartition index: 0, input channels: [InputChannelDeploymentDescriptor [consumed partition id: c388e11d32d1a5ef@26a52e01daa2acbb, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: e052049d23e06b0b@0927bf48e98fa5a1, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 9bfd78352516b342@56e70c43532d8fcf, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 6a55e831d2538c95@449703fe2f7f31e3, consumed partition location: ResultPartitionLocation [UNKNOWN]]]]]]) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$f].
2017-03-27 14:00:37 INFO  Task:446 - Loading JAR files for task Custom Source -> Stream Sink (4/4)
2017-03-27 14:00:37 INFO  Task:825 - Custom Source -> Stream Sink (1/4) switched to RUNNING
2017-03-27 14:00:37 DEBUG SingleInputGate:533 - Created input channels [UnknownInputChannel [c388e11d32d1a5ef@26a52e01daa2acbb], UnknownInputChannel [e052049d23e06b0b@0927bf48e98fa5a1], UnknownInputChannel [9bfd78352516b342@56e70c43532d8fcf], UnknownInputChannel [6a55e831d2538c95@449703fe2f7f31e3]] from InputGateDeploymentDescriptor [result id: 27b2d0891784003b, consumed subpartition index: 0, input channels: [InputChannelDeploymentDescriptor [consumed partition id: c388e11d32d1a5ef@26a52e01daa2acbb, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: e052049d23e06b0b@0927bf48e98fa5a1, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 9bfd78352516b342@56e70c43532d8fcf, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 6a55e831d2538c95@449703fe2f7f31e3, consumed partition location: ResultPartitionLocation [UNKNOWN]]]].
2017-03-27 14:00:37 INFO  TaskManager:128 - Received task Stream Sink (1/1)
2017-03-27 14:00:37 DEBUG SourceStreamTask:48 - Task Custom Source -> Stream Sink invoked
2017-03-27 14:00:37 INFO  Task:825 - Custom Source -> Stream Sink (2/4) switched to RUNNING
2017-03-27 14:00:37 INFO  Task:825 - Custom Source -> Stream Sink (3/4) switched to RUNNING
2017-03-27 14:00:37 DEBUG SourceStreamTask:48 - Task Custom Source -> Stream Sink invoked
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:313 - Kafka consumer 2 will read partitions [] out of partitions [0]
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:319 - Kafka consumer 2 has no partitions (empty source)
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:313 - Kafka consumer 0 will read partitions [fast-messages-0] out of partitions [0]
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message SubmitTask(TaskDeploymentDescriptor [job id: 849a403605bf5d7e8d14df90a927de42, job vertex id: a63137b602209a3152a8545c80149a16, execution id: cf45bb49724013c386aab08613f225dc, task name: Stream Sink (0/1), invokable: org.apache.flink.streaming.runtime.tasks.OneInputStreamTask, produced partitions: [], input gates: [InputGateDeploymentDescriptor [result id: 27b2d0891784003b, consumed subpartition index: 0, input channels: [InputChannelDeploymentDescriptor [consumed partition id: c388e11d32d1a5ef@26a52e01daa2acbb, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: e052049d23e06b0b@0927bf48e98fa5a1, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 9bfd78352516b342@56e70c43532d8fcf, consumed partition location: ResultPartitionLocation [UNKNOWN]], InputChannelDeploymentDescriptor [consumed partition id: 6a55e831d2538c95@449703fe2f7f31e3, consumed partition location: ResultPartitionLocation [UNKNOWN]]]]]]) in 6 ms from Actor[akka://flink/temp/$f].
2017-03-27 14:00:37 DEBUG FileOutputFormat:207 - Opening stream for output (3/4). WriteMode=NO_OVERWRITE, OutputDirectoryMode=PARONLY
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=RUNNING, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=RUNNING, error=(null)) in 1 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=RUNNING, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=RUNNING, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=RUNNING, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$g].
2017-03-27 14:00:37 INFO  Task:446 - Loading JAR files for task Stream Sink (1/1)
2017-03-27 14:00:37 DEBUG Task:679 - Register task cf45bb49724013c386aab08613f225dc at library cache manager took 1 milliseconds
2017-03-27 14:00:37 INFO  Task:463 - Registering task at network: Stream Sink (1/1) [DEPLOYING]
2017-03-27 14:00:37 INFO  StreamTask:131 - State backend for state checkpoints is set to jobmanager.
2017-03-27 14:00:37 DEBUG SourceStreamTask:48 - Task Custom Source -> Stream Sink invoked
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:313 - Kafka consumer 1 will read partitions [] out of partitions [0]
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:319 - Kafka consumer 1 has no partitions (empty source)
2017-03-27 14:00:37 DEBUG FileOutputFormat:207 - Opening stream for output (2/4). WriteMode=NO_OVERWRITE, OutputDirectoryMode=PARONLY
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=RUNNING, error=(null)) in 6 ms from Actor[akka://flink/temp/$g].
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=RUNNING, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$h].
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/temp/$h].
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=RUNNING, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$i].
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/temp/$i].
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (2/4) (de70f918d457c7420927bf48e98fa5a1) switched from DEPLOYING to RUNNING
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb) switched from DEPLOYING to RUNNING
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf) switched from DEPLOYING to RUNNING
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(2/4) switched to RUNNING 
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(1/4) switched to RUNNING 
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(3/4) switched to RUNNING 
2017-03-27 14:00:37 DEBUG HadoopFileSystem:189 - Cannot find hdfs-default configuration file
2017-03-27 14:00:37 DEBUG HadoopFileSystem:196 - Cannot find hdfs-site configuration file
2017-03-27 14:00:37 DEBUG HadoopFileSystem:86 - Trying to load HDFS class Hadoop 2.x style.
2017-03-27 14:00:37 INFO  Task:825 - Stream Sink (1/1) switched to RUNNING
2017-03-27 14:00:37 DEBUG OneInputStreamTask:93 - Task Stream Sink invoked
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=RUNNING, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG Task:679 - Register task ff3a049e2641d373449703fe2f7f31e3 at library cache manager took 0 milliseconds
2017-03-27 14:00:37 INFO  Task:463 - Registering task at network: Custom Source -> Stream Sink (4/4) [DEPLOYING]
2017-03-27 14:00:37 DEBUG ResultPartitionManager:61 - Registered ResultPartition 6a55e831d2538c95@449703fe2f7f31e3 [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:37 INFO  StreamTask:131 - State backend for state checkpoints is set to jobmanager.
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 INFO  Task:825 - Custom Source -> Stream Sink (4/4) switched to RUNNING
2017-03-27 14:00:37 DEBUG SourceStreamTask:48 - Task Custom Source -> Stream Sink invoked
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:313 - Kafka consumer 3 will read partitions [] out of partitions [0]
2017-03-27 14:00:37 INFO  FlinkKafkaConsumer:319 - Kafka consumer 3 has no partitions (empty source)
2017-03-27 14:00:37 DEBUG FileOutputFormat:207 - Opening stream for output (4/4). WriteMode=NO_OVERWRITE, OutputDirectoryMode=PARONLY
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=RUNNING, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$j].
2017-03-27 14:00:37 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=RUNNING, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 DEBUG ZkConnection:63 - Creating new ZookKeeper instance to connect to 10.6.185.142:2181.
2017-03-27 14:00:37 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc) switched from DEPLOYING to RUNNING
2017-03-27 14:00:37 INFO  ZkEventThread:64 - Starting ZkClient event thread.
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Stream Sink(1/1) switched to RUNNING 
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=RUNNING, error=(null)) in 2 ms from Actor[akka://flink/temp/$j].
2017-03-27 14:00:37 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=RUNNING, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$k].
2017-03-27 14:00:37 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=RUNNING, error=(null)) in 0 ms from Actor[akka://flink/temp/$k].
2017-03-27 14:00:37 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3) switched from DEPLOYING to RUNNING
2017-03-27 14:00:37 INFO  JobClient:145 - 03/27/2017 14:00:37	Custom Source -> Stream Sink(4/4) switched to RUNNING 
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:host.name=BLRRIDFWD20597.ITCINFOTECH.com
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.version=1.7.0_60
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.vendor=Oracle Corporation
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.home=C:\Program Files\Java\jdk1.7.0_60\jre
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.class.path=C:\Users\20597\git\zeas08Dec\zdp\Streaming\SparkConsumer\target\classes;C:\Users\20597\.m2\repository\org\apache\spark\spark-streaming_2.10\1.6.0\spark-streaming_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\org\apache\spark\spark-core_2.10\1.6.0\spark-core_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;C:\Users\20597\.m2\repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;C:\Users\20597\.m2\repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;C:\Users\20597\.m2\repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;C:\Users\20597\.m2\repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;C:\Users\20597\.m2\repository\com\twitter\chill_2.10\0.5.0\chill_2.10-0.5.0.jar;C:\Users\20597\.m2\repository\com\twitter\chill-java\0.5.0\chill-java-0.5.0.jar;C:\Users\20597\.m2\repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-client\2.2.0\hadoop-client-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-common\2.2.0\hadoop-common-2.2.0.jar;C:\Users\20597\.m2\repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;C:\Users\20597\.m2\repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\20597\.m2\repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\20597\.m2\repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;C:\Users\20597\.m2\repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-auth\2.2.0\hadoop-auth-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;C:\Users\20597\.m2\repository\org\tukaani\xz\1.0\xz-1.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-hdfs\2.2.0\hadoop-hdfs-2.2.0.jar;C:\Users\20597\.m2\repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.2.0\hadoop-mapreduce-client-app-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.2.0\hadoop-mapreduce-client-common-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-yarn-client\2.2.0\hadoop-yarn-client-2.2.0.jar;C:\Users\20597\.m2\repository\com\google\inject\guice\3.0\guice-3.0.jar;C:\Users\20597\.m2\repository\javax\inject\javax.inject\1\javax.inject-1.jar;C:\Users\20597\.m2\repository\aopalliance\aopalliance\1.0\aopalliance-1.0.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-test-framework\jersey-test-framework-grizzly2\1.9\jersey-test-framework-grizzly2-1.9.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-test-framework\jersey-test-framework-core\1.9\jersey-test-framework-core-1.9.jar;C:\Users\20597\.m2\repository\javax\servlet\javax.servlet-api\3.0.1\javax.servlet-api-3.0.1.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-grizzly2\1.9\jersey-grizzly2-1.9.jar;C:\Users\20597\.m2\repository\org\glassfish\grizzly\grizzly-http\2.1.2\grizzly-http-2.1.2.jar;C:\Users\20597\.m2\repository\org\glassfish\grizzly\grizzly-framework\2.1.2\grizzly-framework-2.1.2.jar;C:\Users\20597\.m2\repository\org\glassfish\gmbal\gmbal-api-only\3.0.0-b023\gmbal-api-only-3.0.0-b023.jar;C:\Users\20597\.m2\repository\org\glassfish\external\management-api\3.0.0-b012\management-api-3.0.0-b012.jar;C:\Users\20597\.m2\repository\org\glassfish\grizzly\grizzly-http-server\2.1.2\grizzly-http-server-2.1.2.jar;C:\Users\20597\.m2\repository\org\glassfish\grizzly\grizzly-rcm\2.1.2\grizzly-rcm-2.1.2.jar;C:\Users\20597\.m2\repository\org\glassfish\grizzly\grizzly-http-servlet\2.1.2\grizzly-http-servlet-2.1.2.jar;C:\Users\20597\.m2\repository\org\glassfish\javax.servlet\3.1\javax.servlet-3.1.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-json\1.9\jersey-json-1.9.jar;C:\Users\20597\.m2\repository\com\sun\xml\bind\jaxb-impl\2.2.3-1\jaxb-impl-2.2.3-1.jar;C:\Users\20597\.m2\repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\20597\.m2\repository\javax\activation\activation\1.1\activation-1.1.jar;C:\Users\20597\.m2\repository\org\codehaus\jackson\jackson-jaxrs\1.8.3\jackson-jaxrs-1.8.3.jar;C:\Users\20597\.m2\repository\org\codehaus\jackson\jackson-xc\1.8.3\jackson-xc-1.8.3.jar;C:\Users\20597\.m2\repository\com\sun\jersey\contribs\jersey-guice\1.9\jersey-guice-1.9.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-yarn-server-common\2.2.0\hadoop-yarn-server-common-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.2.0\hadoop-mapreduce-client-shuffle-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-yarn-api\2.2.0\hadoop-yarn-api-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.2.0\hadoop-mapreduce-client-core-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-yarn-common\2.2.0\hadoop-yarn-common-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.2.0\hadoop-mapreduce-client-jobclient-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\hadoop\hadoop-annotations\2.2.0\hadoop-annotations-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\spark\spark-launcher_2.10\1.6.0\spark-launcher_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\org\apache\spark\spark-network-common_2.10\1.6.0\spark-network-common_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\org\apache\spark\spark-network-shuffle_2.10\1.6.0\spark-network-shuffle_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;C:\Users\20597\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.4.4\jackson-annotations-2.4.4.jar;C:\Users\20597\.m2\repository\org\apache\spark\spark-unsafe_2.10\1.6.0\spark-unsafe_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;C:\Users\20597\.m2\repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;C:\Users\20597\.m2\repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\20597\.m2\repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;C:\Users\20597\.m2\repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;C:\Users\20597\.m2\repository\org\apache\curator\curator-client\2.4.0\curator-client-2.4.0.jar;C:\Users\20597\.m2\repository\org\apache\zookeeper\zookeeper\3.4.5\zookeeper-3.4.5.jar;C:\Users\20597\.m2\repository\jline\jline\0.9.94\jline-0.9.94.jar;C:\Users\20597\.m2\repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\orbit\javax.servlet\3.0.0.v201112011016\javax.servlet-3.0.0.v201112011016.jar;C:\Users\20597\.m2\repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\20597\.m2\repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;C:\Users\20597\.m2\repository\org\slf4j\jul-to-slf4j\1.7.10\jul-to-slf4j-1.7.10.jar;C:\Users\20597\.m2\repository\org\slf4j\jcl-over-slf4j\1.7.10\jcl-over-slf4j-1.7.10.jar;C:\Users\20597\.m2\repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;C:\Users\20597\.m2\repository\org\xerial\snappy\snappy-java\1.1.2\snappy-java-1.1.2.jar;C:\Users\20597\.m2\repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;C:\Users\20597\.m2\repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;C:\Users\20597\.m2\repository\commons-net\commons-net\2.2\commons-net-2.2.jar;C:\Users\20597\.m2\repository\com\typesafe\akka\akka-remote_2.10\2.3.11\akka-remote_2.10-2.3.11.jar;C:\Users\20597\.m2\repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;C:\Users\20597\.m2\repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\Users\20597\.m2\repository\org\uncommons\maths\uncommons-maths\1.2.2a\uncommons-maths-1.2.2a.jar;C:\Users\20597\.m2\repository\com\typesafe\akka\akka-slf4j_2.10\2.3.11\akka-slf4j_2.10-2.3.11.jar;C:\Users\20597\.m2\repository\org\json4s\json4s-jackson_2.10\3.2.10\json4s-jackson_2.10-3.2.10.jar;C:\Users\20597\.m2\repository\org\json4s\json4s-core_2.10\3.2.10\json4s-core_2.10-3.2.10.jar;C:\Users\20597\.m2\repository\org\json4s\json4s-ast_2.10\3.2.10\json4s-ast_2.10-3.2.10.jar;C:\Users\20597\.m2\repository\org\scala-lang\scalap\2.10.0\scalap-2.10.0.jar;C:\Users\20597\.m2\repository\org\scala-lang\scala-compiler\2.10.0\scala-compiler-2.10.0.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-server\1.9\jersey-server-1.9.jar;C:\Users\20597\.m2\repository\asm\asm\3.1\asm-3.1.jar;C:\Users\20597\.m2\repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;C:\Users\20597\.m2\repository\org\apache\mesos\mesos\0.21.1\mesos-0.21.1-shaded-protobuf.jar;C:\Users\20597\.m2\repository\io\netty\netty-all\4.0.29.Final\netty-all-4.0.29.Final.jar;C:\Users\20597\.m2\repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;C:\Users\20597\.m2\repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;C:\Users\20597\.m2\repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;C:\Users\20597\.m2\repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;C:\Users\20597\.m2\repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;C:\Users\20597\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.4.4\jackson-databind-2.4.4.jar;C:\Users\20597\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.4.4\jackson-core-2.4.4.jar;C:\Users\20597\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.10\2.4.4\jackson-module-scala_2.10-2.4.4.jar;C:\Users\20597\.m2\repository\org\scala-lang\scala-reflect\2.10.4\scala-reflect-2.10.4.jar;C:\Users\20597\.m2\repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;C:\Users\20597\.m2\repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;C:\Users\20597\.m2\repository\oro\oro\2.0.8\oro-2.0.8.jar;C:\Users\20597\.m2\repository\org\tachyonproject\tachyon-client\0.8.2\tachyon-client-0.8.2.jar;C:\Users\20597\.m2\repository\commons-lang\commons-lang\2.4\commons-lang-2.4.jar;C:\Users\20597\.m2\repository\org\tachyonproject\tachyon-underfs-hdfs\0.8.2\tachyon-underfs-hdfs-0.8.2.jar;C:\Users\20597\.m2\repository\org\tachyonproject\tachyon-underfs-s3\0.8.2\tachyon-underfs-s3-0.8.2.jar;C:\Users\20597\.m2\repository\org\tachyonproject\tachyon-underfs-local\0.8.2\tachyon-underfs-local-0.8.2.jar;C:\Users\20597\.m2\repository\net\razorvine\pyrolite\4.9\pyrolite-4.9.jar;C:\Users\20597\.m2\repository\net\sf\py4j\py4j\0.9\py4j-0.9.jar;C:\Users\20597\.m2\repository\org\scala-lang\scala-library\2.10.5\scala-library-2.10.5.jar;C:\Users\20597\.m2\repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;C:\Users\20597\.m2\repository\org\apache\spark\spark-streaming-kafka_2.10\1.6.0\spark-streaming-kafka_2.10-1.6.0.jar;C:\Users\20597\.m2\repository\org\apache\kafka\kafka_2.10\0.8.2.1\kafka_2.10-0.8.2.1.jar;C:\Users\20597\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\20597\.m2\repository\org\apache\kafka\kafka-clients\0.8.2.1\kafka-clients-0.8.2.1.jar;C:\Users\20597\.m2\repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-streaming-core\0.9.1\flink-streaming-core-0.9.1.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-core\0.9.1\flink-core-0.9.1.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-shaded-include-yarn\0.9.1\flink-shaded-include-yarn-0.9.1.jar;C:\Users\20597\.m2\repository\commons-collections\commons-collections\3.2.1\commons-collections-3.2.1.jar;C:\Users\20597\.m2\repository\com\esotericsoftware\kryo\kryo\2.24.0\kryo-2.24.0.jar;C:\Users\20597\.m2\repository\com\esotericsoftware\minlog\minlog\1.2\minlog-1.2.jar;C:\Users\20597\.m2\repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-runtime\0.9.1\flink-runtime-0.9.1.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-java\0.9.1\flink-java-0.9.1.jar;C:\Users\20597\.m2\repository\org\apache\avro\avro\1.7.6\avro-1.7.6.jar;C:\Users\20597\.m2\repository\com\twitter\chill-avro_2.10\0.5.2\chill-avro_2.10-0.5.2.jar;C:\Users\20597\.m2\repository\com\twitter\chill-bijection_2.10\0.5.2\chill-bijection_2.10-0.5.2.jar;C:\Users\20597\.m2\repository\com\twitter\bijection-core_2.10\0.7.2\bijection-core_2.10-0.7.2.jar;C:\Users\20597\.m2\repository\com\twitter\bijection-avro_2.10\0.7.2\bijection-avro_2.10-0.7.2.jar;C:\Users\20597\.m2\repository\de\javakaffee\kryo-serializers\0.27\kryo-serializers-0.27.jar;C:\Users\20597\.m2\repository\joda-time\joda-time\2.5\joda-time-2.5.jar;C:\Users\20597\.m2\repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;C:\Users\20597\.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-server\8.0.0.M1\jetty-server-8.0.0.M1.jar;C:\Users\20597\.m2\repository\org\mortbay\jetty\servlet-api\3.0.20100224\servlet-api-3.0.20100224.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-continuation\8.0.0.M1\jetty-continuation-8.0.0.M1.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-http\8.0.0.M1\jetty-http-8.0.0.M1.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-io\8.0.0.M1\jetty-io-8.0.0.M1.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-util\8.0.0.M1\jetty-util-8.0.0.M1.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-security\8.0.0.M1\jetty-security-8.0.0.M1.jar;C:\Users\20597\.m2\repository\org\eclipse\jetty\jetty-servlet\8.0.0.M1\jetty-servlet-8.0.0.M1.jar;C:\Users\20597\.m2\repository\com\amazonaws\aws-java-sdk\1.8.1\aws-java-sdk-1.8.1.jar;C:\Users\20597\.m2\repository\commons-logging\commons-logging\1.1.1\commons-logging-1.1.1.jar;C:\Users\20597\.m2\repository\org\apache\httpcomponents\httpclient\4.2\httpclient-4.2.jar;C:\Users\20597\.m2\repository\org\apache\httpcomponents\httpcore\4.2\httpcore-4.2.jar;C:\Users\20597\.m2\repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;C:\Users\20597\.m2\repository\org\codehaus\jettison\jettison\1.1\jettison-1.1.jar;C:\Users\20597\.m2\repository\stax\stax-api\1.0.1\stax-api-1.0.1.jar;C:\Users\20597\.m2\repository\com\typesafe\akka\akka-actor_2.10\2.3.7\akka-actor_2.10-2.3.7.jar;C:\Users\20597\.m2\repository\com\typesafe\config\1.2.1\config-1.2.1.jar;C:\Users\20597\.m2\repository\org\clapper\grizzled-slf4j_2.10\1.0.2\grizzled-slf4j_2.10-1.0.2.jar;C:\Users\20597\.m2\repository\com\github\scopt\scopt_2.10\3.2.0\scopt_2.10-3.2.0.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-clients\0.9.1\flink-clients-0.9.1.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-optimizer\0.9.1\flink-optimizer-0.9.1.jar;C:\Users\20597\.m2\repository\commons-fileupload\commons-fileupload\1.3.1\commons-fileupload-1.3.1.jar;C:\Users\20597\.m2\repository\org\apache\commons\commons-math\2.2\commons-math-2.2.jar;C:\Users\20597\.m2\repository\org\apache\sling\org.apache.sling.commons.json\2.0.6\org.apache.sling.commons.json-2.0.6.jar;C:\Users\20597\.m2\repository\org\apache\commons\commons-lang3\3.3.2\commons-lang3-3.3.2.jar;C:\Users\20597\.m2\repository\org\slf4j\slf4j-api\1.7.7\slf4j-api-1.7.7.jar;C:\Users\20597\.m2\repository\org\slf4j\slf4j-log4j12\1.7.7\slf4j-log4j12-1.7.7.jar;C:\Users\20597\.m2\repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\20597\.m2\repository\org\apache\flink\flink-connector-kafka\0.9.1\flink-connector-kafka-0.9.1.jar
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.library.path=C:\Program Files\Java\jdk1.7.0_60\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Program Files (x86)\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Program Files (x86)\Windows Live\Shared;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\CCM;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\MySQL\MySQL Workbench 6.3 CE;C:\Program Files\VisualSVN Server\bin;C:\Program Files\TortoiseSVN\bin;C:\Program Files\Git\cmd;C:\Program Files\MongoDB\Server\3.2\bin;D:\Softwares\apache-maven-3.2.3\bin;D:\Git\cmd;.
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.io.tmpdir=C:\Users\20597\AppData\Local\Temp\
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:java.compiler=<NA>
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:os.name=Windows 7
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:os.arch=amd64
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:os.version=6.1
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:user.name=20597
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:user.home=C:\Users\20597
2017-03-27 14:00:37 INFO  ZooKeeper:100 - Client environment:user.dir=C:\Users\20597\git\zeas08Dec\zdp\Streaming\SparkConsumer
2017-03-27 14:00:37 INFO  ZooKeeper:438 - Initiating client connection, connectString=10.6.185.142:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@6af84412
2017-03-27 14:00:37 DEBUG ClientCnxn:99 - zookeeper.disableAutoWatchReset is false
2017-03-27 14:00:37 DEBUG ZkClient:878 - Awaiting connection to Zookeeper server
2017-03-27 14:00:37 DEBUG ZkClient:628 - Waiting for keeper state SyncConnected
2017-03-27 14:00:37 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, about=, value=[Rate of successful kerberos logins and latency (milliseconds)], always=false, type=DEFAULT, sampleName=Ops)
2017-03-27 14:00:37 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, about=, value=[Rate of failed kerberos logins and latency (milliseconds)], always=false, type=DEFAULT, sampleName=Ops)
2017-03-27 14:00:37 DEBUG MetricsSystemImpl:220 - UgiMetrics, User and group related metrics
2017-03-27 14:00:37 DEBUG Groups:180 -  Creating new Groups object
2017-03-27 14:00:37 DEBUG NativeCodeLoader:46 - Trying to load the custom-built native-hadoop library...
2017-03-27 14:00:37 DEBUG NativeCodeLoader:55 - Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2017-03-27 14:00:37 DEBUG NativeCodeLoader:56 - java.library.path=C:\Program Files\Java\jdk1.7.0_60\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Program Files (x86)\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Program Files (x86)\Windows Live\Shared;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\CCM;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\MySQL\MySQL Workbench 6.3 CE;C:\Program Files\VisualSVN Server\bin;C:\Program Files\TortoiseSVN\bin;C:\Program Files\Git\cmd;C:\Program Files\MongoDB\Server\3.2\bin;D:\Softwares\apache-maven-3.2.3\bin;D:\Git\cmd;.
2017-03-27 14:00:37 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-03-27 14:00:37 DEBUG JniBasedUnixGroupsMappingWithFallback:40 - Falling back to shell based
2017-03-27 14:00:37 DEBUG JniBasedUnixGroupsMappingWithFallback:44 - Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2017-03-27 14:00:38 DEBUG Groups:66 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000
2017-03-27 14:00:38 DEBUG UserGroupInformation:177 - hadoop login
2017-03-27 14:00:38 DEBUG UserGroupInformation:126 - hadoop login commit
2017-03-27 14:00:38 DEBUG UserGroupInformation:156 - using local user:NTUserPrincipal: 20597
2017-03-27 14:00:38 DEBUG UserGroupInformation:703 - UGI loginUser:20597 (auth:SIMPLE)
2017-03-27 14:00:38 DEBUG :356 - address: BLRRIDFWD20597/10.6.185.23 isLoopbackAddress: false, with host 10.6.185.23 BLRRIDFWD20597
2017-03-27 14:00:39 WARN  :389 - Your hostname, BLRRIDFWD20597 resolves to a loopback/non-reachable address: 192.168.129.1, but we couldn't find any external IP address!
2017-03-27 14:00:39 DEBUG InternalLoggerFactory:71 - Using SLF4J as the default logging framework
2017-03-27 14:00:39 DEBUG HadoopFileSystem:102 - Loaded 'org.apache.hadoop.hdfs.DistributedFileSystem' as HDFS class.
2017-03-27 14:00:39 DEBUG BlockReaderLocal:334 - dfs.client.use.legacy.blockreader.local = false
2017-03-27 14:00:39 DEBUG BlockReaderLocal:337 - dfs.client.read.shortcircuit = false
2017-03-27 14:00:39 DEBUG BlockReaderLocal:340 - dfs.client.domain.socket.data.traffic = false
2017-03-27 14:00:39 DEBUG BlockReaderLocal:343 - dfs.domain.socket.path = 
2017-03-27 14:00:39 DEBUG MetricsSystemImpl:220 - StartupProgress, NameNode startup progress
2017-03-27 14:00:39 DEBUG RetryUtils:74 - multipleLinearRandomRetry = null
2017-03-27 14:00:39 DEBUG Server:220 - rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@643e2477
2017-03-27 14:00:39 DEBUG BlockReaderLocal:63 - Both short-circuit local reads and UNIX domain socket are disabled.
2017-03-27 14:00:39 DEBUG Shell:243 - Failed to detect a valid hadoop home directory
java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set.
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:225)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:250)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
	at org.apache.hadoop.conf.Configuration.getTrimmedStrings(Configuration.java:1546)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:519)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:453)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.initialize(HadoopFileSystem.java:313)
	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:258)
	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:309)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:216)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
2017-03-27 14:00:39 ERROR Shell:303 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
	at org.apache.hadoop.conf.Configuration.getTrimmedStrings(Configuration.java:1546)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:519)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:453)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.initialize(HadoopFileSystem.java:313)
	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:258)
	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:309)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:216)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
2017-03-27 14:00:42 DEBUG TaskManager:86 - Received message SendHeartbeat at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:42 DEBUG TaskManager:86 - Sending heartbeat to JobManager
2017-03-27 14:00:42 INFO  ClientCnxn:966 - Opening socket connection to server 10.6.185.142/10.6.185.142:2181. Will not attempt to authenticate using SASL (unknown error)
2017-03-27 14:00:42 INFO  ClientCnxn:849 - Socket connection established to 10.6.185.142/10.6.185.142:2181, initiating session
2017-03-27 14:00:42 DEBUG ClientCnxn:889 - Session establishment request sent on 10.6.185.142/10.6.185.142:2181
2017-03-27 14:00:42 INFO  ClientCnxn:1207 - Session establishment complete on server 10.6.185.142/10.6.185.142:2181, sessionid = 0x15a92c9294001cb, negotiated timeout = 6000
2017-03-27 14:00:42 DEBUG ZkClient:351 - Received event: WatchedEvent state:SyncConnected type:None path:null
2017-03-27 14:00:42 INFO  ZkClient:449 - zookeeper state changed (SyncConnected)
2017-03-27 14:00:42 DEBUG ZkClient:395 - Leaving process event
2017-03-27 14:00:42 DEBUG ZkClient:638 - State is SyncConnected
2017-03-27 14:00:42 DEBUG ClientCnxn:815 - Reading reply sessionid:0x15a92c9294001cb, packet:: clientPath:null serverPath:null finished:false header:: 1,4  replyHeader:: 1,858993479976,0  request:: '/consumers/test/offsets/fast-messages/0,F  response:: #3131373536353832,s{858993462635,858993462644,1488812420017,1488812869355,1,0,0,0,8,0,858993462635} 
2017-03-27 14:00:42 INFO  ZookeeperOffsetHandler:91 - Offset for partition 0 was set to 11756582 in ZooKeeper. Seeking fetcher to that position.
2017-03-27 14:00:42 DEBUG FileOutputFormat:207 - Opening stream for output (1/4). WriteMode=NO_OVERWRITE, OutputDirectoryMode=PARONLY
2017-03-27 14:00:42 DEBUG TaskManager:86 - Handled message SendHeartbeat in 204 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:42 DEBUG JobManager:86 - Received message Heartbeat(7f747259085967c272ac26bfa10c5a15,[B@33571678) at akka://flink/user/jobmanager from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:42 DEBUG JobManager:86 - Received hearbeat message from 7f747259085967c272ac26bfa10c5a15.
2017-03-27 14:00:42 DEBUG InstanceManager:121 - Received heartbeat from TaskManager 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1
2017-03-27 14:00:42 DEBUG JobManager:86 - Handled message Heartbeat(7f747259085967c272ac26bfa10c5a15,[B@33571678) in 1 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:43 DEBUG DFSClient:1371 - /user/zeas/stream/flink/3: masked=rw-r--r--
2017-03-27 14:00:43 DEBUG DFSClient:1371 - /user/zeas/stream/flink/2: masked=rw-r--r--
2017-03-27 14:00:43 DEBUG DFSClient:1371 - /user/zeas/stream/flink/4: masked=rw-r--r--
2017-03-27 14:00:44 DEBUG Client:371 - The ping interval is 60000 ms.
2017-03-27 14:00:44 DEBUG Client:636 - Connecting to /10.6.185.142:8020
2017-03-27 14:00:44 DEBUG Client:886 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597: starting, having connections 1
2017-03-27 14:00:44 DEBUG Client:948 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 sending #1
2017-03-27 14:00:44 DEBUG Client:948 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 sending #2
2017-03-27 14:00:44 DEBUG Client:948 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 sending #0
2017-03-27 14:00:44 DEBUG Client:1005 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 got value #1
2017-03-27 14:00:44 DEBUG Client:1005 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 got value #2
2017-03-27 14:00:44 DEBUG Client:1005 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 got value #0
2017-03-27 14:00:44 ERROR SourceStreamTask:66 - Custom Source -> Stream Sink (2/4) failed
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 ERROR SourceStreamTask:66 - Custom Source -> Stream Sink (3/4) failed
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 ERROR SourceStreamTask:66 - Custom Source -> Stream Sink (4/4) failed
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 INFO  Task:828 - Custom Source -> Stream Sink (4/4) switched to FAILED with exception.
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 INFO  Task:828 - Custom Source -> Stream Sink (2/4) switched to FAILED with exception.
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 INFO  Task:647 - Freeing task resources for Custom Source -> Stream Sink (4/4)
2017-03-27 14:00:44 INFO  Task:647 - Freeing task resources for Custom Source -> Stream Sink (2/4)
2017-03-27 14:00:44 DEBUG NetworkEnvironment:333 - Unregister task Custom Source -> Stream Sink (2/4) from network environment (state: FAILED).
2017-03-27 14:00:44 DEBUG NetworkEnvironment:333 - Unregister task Custom Source -> Stream Sink (4/4) from network environment (state: FAILED).
2017-03-27 14:00:44 DEBUG ResultPartition:289 - Custom Source -> Stream Sink (2/4) (de70f918d457c7420927bf48e98fa5a1): Releasing ResultPartition e052049d23e06b0b@0927bf48e98fa5a1 [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:44 DEBUG PipelinedSubpartition:148 - Released PipelinedSubpartition [number of buffers: 0 (0 bytes), finished? false, read view? false].
2017-03-27 14:00:44 DEBUG ResultPartitionManager:104 - Released all partitions produced by de70f918d457c7420927bf48e98fa5a1.
2017-03-27 14:00:44 DEBUG ResultPartition:289 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3): Releasing ResultPartition 6a55e831d2538c95@449703fe2f7f31e3 [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:44 DEBUG PipelinedSubpartition:148 - Released PipelinedSubpartition [number of buffers: 0 (0 bytes), finished? false, read view? false].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message TaskInFinalState(de70f918d457c7420927bf48e98fa5a1) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG ResultPartitionManager:104 - Released all partitions produced by ff3a049e2641d373449703fe2f7f31e3.
2017-03-27 14:00:44 INFO  TaskManager:128 - Unregistering task and sending final execution state FAILED to JobManager for task Custom Source -> Stream Sink (de70f918d457c7420927bf48e98fa5a1)
2017-03-27 14:00:44 INFO  Task:828 - Custom Source -> Stream Sink (3/4) switched to FAILED with exception.
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message TaskInFinalState(de70f918d457c7420927bf48e98fa5a1) in 2 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 INFO  Task:647 - Freeing task resources for Custom Source -> Stream Sink (3/4)
2017-03-27 14:00:44 DEBUG NetworkEnvironment:333 - Unregister task Custom Source -> Stream Sink (3/4) from network environment (state: FAILED).
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message TaskInFinalState(ff3a049e2641d373449703fe2f7f31e3) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG ResultPartition:289 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf): Releasing ResultPartition 9bfd78352516b342@56e70c43532d8fcf [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:44 INFO  TaskManager:128 - Unregistering task and sending final execution state FAILED to JobManager for task Custom Source -> Stream Sink (ff3a049e2641d373449703fe2f7f31e3)
2017-03-27 14:00:44 DEBUG PipelinedSubpartition:148 - Released PipelinedSubpartition [number of buffers: 0 (0 bytes), finished? false, read view? false].
2017-03-27 14:00:44 DEBUG ResultPartitionManager:104 - Released all partitions produced by b5231155e7318a7556e70c43532d8fcf.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message TaskInFinalState(ff3a049e2641d373449703fe2f7f31e3) in 1 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) at akka://flink/user/taskmanager_1 from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) in 0 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$l].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message TaskInFinalState(b5231155e7318a7556e70c43532d8fcf) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 INFO  TaskManager:128 - Unregistering task and sending final execution state FAILED to JobManager for task Custom Source -> Stream Sink (b5231155e7318a7556e70c43532d8fcf)
2017-03-27 14:00:44 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=de70f918d457c7420927bf48e98fa5a1, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) in 0 ms from Actor[akka://flink/temp/$l].
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (2/4) (de70f918d457c7420927bf48e98fa5a1) switched from RUNNING to FAILED
2017-03-27 14:00:44 DEBUG ExecutionGraph:667 - Socket Window WordCount switched from RUNNING to FAILING.
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Custom Source -> Stream Sink(2/4) switched to FAILED 
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more

2017-03-27 14:00:44 DEBUG JobManager:86 - Received message 03/27/2017 14:00:44	Job execution switched to status FAILING. at akka://flink/user/jobmanager from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Job execution switched to status FAILING.
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb) switched from RUNNING to CANCELING
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message TaskInFinalState(b5231155e7318a7556e70c43532d8fcf) in 2 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Custom Source -> Stream Sink(1/4) switched to CANCELING 
2017-03-27 14:00:44 INFO  JobManager:137 - Status of job 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount) changed to FAILING.
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) at akka://flink/user/taskmanager_1 from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 DEBUG JobManager:86 - Handled message 03/27/2017 14:00:44	Job execution switched to status FAILING. in 1 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) in 0 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$m].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) at akka://flink/user/taskmanager_1 from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf) switched from RUNNING to CANCELING
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Custom Source -> Stream Sink(3/4) switched to CANCELING 
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) in 0 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3) switched from RUNNING to CANCELING
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (4/4) (ff3a049e2641d373449703fe2f7f31e3) switched from CANCELING to FAILED
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message CancelTask(867c029d7acfa61e26a52e01daa2acbb) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$n].
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc) switched from RUNNING to CANCELING
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Custom Source -> Stream Sink(4/4) switched to CANCELING 
2017-03-27 14:00:44 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=ff3a049e2641d373449703fe2f7f31e3, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) in 0 ms from Actor[akka://flink/temp/$m].
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Custom Source -> Stream Sink(4/4) switched to FAILED 
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/4":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more

2017-03-27 14:00:44 INFO  Task:747 - Attempting to cancel task Custom Source -> Stream Sink (1/4)
2017-03-27 14:00:44 INFO  Task:825 - Custom Source -> Stream Sink (1/4) switched to CANCELING
2017-03-27 14:00:44 DEBUG SlotSharingGroupAssignment:492 - Release simple slot SimpleSlot (3)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Stream Sink(1/1) switched to CANCELING 
2017-03-27 14:00:44 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$o].
2017-03-27 14:00:44 INFO  Task:792 - Triggering cancellation of task code Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb).
2017-03-27 14:00:44 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=b5231155e7318a7556e70c43532d8fcf, state=FAILED, error=org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)
) in 0 ms from Actor[akka://flink/temp/$o].
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (3/4) (b5231155e7318a7556e70c43532d8fcf) switched from CANCELING to FAILED
2017-03-27 14:00:44 DEBUG SlotSharingGroupAssignment:492 - Release simple slot SimpleSlot (1)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Custom Source -> Stream Sink(3/4) switched to FAILED 
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/3":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more

2017-03-27 14:00:44 DEBUG ZkClient:919 - Closing ZkClient...
2017-03-27 14:00:44 DEBUG Instance:293 - Return allocated slot Shared (1) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message CancelTask(867c029d7acfa61e26a52e01daa2acbb) in 3 ms from Actor[akka://flink/temp/$n].
2017-03-27 14:00:44 INFO  ZkEventThread:82 - Terminate ZkClient event thread.
2017-03-27 14:00:44 DEBUG ZkConnection:77 - Closing ZooKeeper connected to 10.6.185.142:2181
2017-03-27 14:00:44 DEBUG ZooKeeper:673 - Closing session: 0x15a92c9294001cb
2017-03-27 14:00:44 DEBUG ClientCnxn:1273 - Closing client for session: 0x15a92c9294001cb
2017-03-27 14:00:44 DEBUG SlotSharingGroupAssignment:492 - Release simple slot SimpleSlot (2)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message FailIntermediateResultPartitions(de70f918d457c7420927bf48e98fa5a1) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG Instance:293 - Return allocated slot Shared (2) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 INFO  TaskManager:128 - Discarding the results produced by task execution de70f918d457c7420927bf48e98fa5a1
2017-03-27 14:00:44 DEBUG ResultPartitionManager:104 - Released all partitions produced by de70f918d457c7420927bf48e98fa5a1.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message FailIntermediateResultPartitions(de70f918d457c7420927bf48e98fa5a1) in 1 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message CancelTask(b5231155e7318a7556e70c43532d8fcf) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$p].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Cannot find task to cancel for execution b5231155e7318a7556e70c43532d8fcf)
2017-03-27 14:00:44 DEBUG ExecutionGraph:825 - Cancel task call did not find task. Probably akka message call race.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message CancelTask(b5231155e7318a7556e70c43532d8fcf) in 0 ms from Actor[akka://flink/temp/$p].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message CancelTask(ff3a049e2641d373449703fe2f7f31e3) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$q].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Cannot find task to cancel for execution ff3a049e2641d373449703fe2f7f31e3)
2017-03-27 14:00:44 DEBUG ExecutionGraph:825 - Cancel task call did not find task. Probably akka message call race.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message CancelTask(ff3a049e2641d373449703fe2f7f31e3) in 0 ms from Actor[akka://flink/temp/$q].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message CancelTask(cf45bb49724013c386aab08613f225dc) at akka://flink/user/taskmanager_1 from Actor[akka://flink/temp/$r].
2017-03-27 14:00:44 INFO  Task:747 - Attempting to cancel task Stream Sink (1/1)
2017-03-27 14:00:44 INFO  Task:825 - Stream Sink (1/1) switched to CANCELING
2017-03-27 14:00:44 INFO  Task:792 - Triggering cancellation of task code Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc).
2017-03-27 14:00:44 DEBUG OneInputStreamTask:109 - Task Stream Sink invocation finished
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message CancelTask(cf45bb49724013c386aab08613f225dc) in 0 ms from Actor[akka://flink/temp/$r].
2017-03-27 14:00:44 INFO  Task:825 - Stream Sink (1/1) switched to CANCELED
2017-03-27 14:00:44 INFO  Task:647 - Freeing task resources for Stream Sink (1/1)
2017-03-27 14:00:44 DEBUG NetworkEnvironment:333 - Unregister task Stream Sink (1/1) from network environment (state: CANCELED).
2017-03-27 14:00:44 DEBUG ResultPartitionManager:104 - Released all partitions produced by cf45bb49724013c386aab08613f225dc.
2017-03-27 14:00:44 DEBUG SingleInputGate:324 - Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc): Releasing org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate@4bc22fd6.
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message TaskInFinalState(cf45bb49724013c386aab08613f225dc) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 INFO  TaskManager:128 - Unregistering task and sending final execution state CANCELED to JobManager for task Stream Sink (cf45bb49724013c386aab08613f225dc)
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message TaskInFinalState(cf45bb49724013c386aab08613f225dc) in 0 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=CANCELED, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=CANCELED, error=(null)) in 0 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:44 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=CANCELED, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$s].
2017-03-27 14:00:44 INFO  ExecutionGraph:882 - Stream Sink (1/1) (cf45bb49724013c386aab08613f225dc) switched from CANCELING to CANCELED
2017-03-27 14:00:44 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=cf45bb49724013c386aab08613f225dc, state=CANCELED, error=(null)) in 0 ms from Actor[akka://flink/temp/$s].
2017-03-27 14:00:44 DEBUG SlotSharingGroupAssignment:492 - Release simple slot SimpleSlot (3)(1) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 INFO  JobClient:145 - 03/27/2017 14:00:44	Stream Sink(1/1) switched to CANCELED 
2017-03-27 14:00:44 DEBUG Instance:293 - Return allocated slot Shared (3) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:44 DEBUG ClientCnxn:815 - Reading reply sessionid:0x15a92c9294001cb, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,858993479977,0  request:: null response:: null
2017-03-27 14:00:44 DEBUG ClientCnxn:1257 - Disconnecting client for session: 0x15a92c9294001cb
2017-03-27 14:00:44 DEBUG ClientCnxn:1073 - An exception was thrown while closing send thread for session 0x15a92c9294001cb : Unable to read additional data from server sessionid 0x15a92c9294001cb, likely server has closed socket
2017-03-27 14:00:44 INFO  ZooKeeper:684 - Session: 0x15a92c9294001cb closed
2017-03-27 14:00:44 DEBUG ZkClient:932 - Closing ZkClient...done
2017-03-27 14:00:44 INFO  ClientCnxn:509 - EventThread shut down
2017-03-27 14:00:47 DEBUG DFSClient:1371 - /user/zeas/stream/flink/1: masked=rw-r--r--
2017-03-27 14:00:47 DEBUG Client:948 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 sending #3
2017-03-27 14:00:47 WARN  Client:1325 - interrupted waiting to send rpc request to server
java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:970)
	at org.apache.hadoop.ipc.Client.call(Client.java:1320)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
2017-03-27 14:00:47 ERROR SourceStreamTask:66 - Custom Source -> Stream Sink (1/4) failed
java.io.IOException: java.lang.InterruptedException
	at org.apache.hadoop.ipc.Client.call(Client.java:1326)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:970)
	at org.apache.hadoop.ipc.Client.call(Client.java:1320)
	... 32 more
2017-03-27 14:00:47 DEBUG Client:1005 - IPC Client (962430423) connection to /10.6.185.142:8020 from 20597 got value #3
2017-03-27 14:00:47 INFO  Task:825 - Custom Source -> Stream Sink (1/4) switched to CANCELED
2017-03-27 14:00:47 INFO  Task:647 - Freeing task resources for Custom Source -> Stream Sink (1/4)
2017-03-27 14:00:47 DEBUG NetworkEnvironment:333 - Unregister task Custom Source -> Stream Sink (1/4) from network environment (state: CANCELED).
2017-03-27 14:00:47 DEBUG ResultPartition:289 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb): Releasing ResultPartition c388e11d32d1a5ef@26a52e01daa2acbb [PIPELINED, 1 subpartitions, 1 pending references].
2017-03-27 14:00:47 DEBUG PipelinedSubpartition:148 - Released PipelinedSubpartition [number of buffers: 0 (0 bytes), finished? false, read view? false].
2017-03-27 14:00:47 DEBUG ResultPartitionManager:104 - Released all partitions produced by 867c029d7acfa61e26a52e01daa2acbb.
2017-03-27 14:00:47 DEBUG TaskManager:86 - Received message TaskInFinalState(867c029d7acfa61e26a52e01daa2acbb) at akka://flink/user/taskmanager_1 from Actor[akka://flink/deadLetters].
2017-03-27 14:00:47 INFO  TaskManager:128 - Unregistering task and sending final execution state CANCELED to JobManager for task Custom Source -> Stream Sink (867c029d7acfa61e26a52e01daa2acbb)
2017-03-27 14:00:47 DEBUG TaskManager:86 - Handled message TaskInFinalState(867c029d7acfa61e26a52e01daa2acbb) in 0 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:47 DEBUG TaskManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=CANCELED, error=(null)) at akka://flink/user/taskmanager_1 from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:47 DEBUG TaskManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=CANCELED, error=(null)) in 0 ms from Actor[akka://flink/user/taskmanager_1#935127370].
2017-03-27 14:00:47 DEBUG JobManager:86 - Received message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=CANCELED, error=(null)) at akka://flink/user/jobmanager from Actor[akka://flink/temp/$t].
2017-03-27 14:00:47 INFO  ExecutionGraph:882 - Custom Source -> Stream Sink (1/4) (867c029d7acfa61e26a52e01daa2acbb) switched from CANCELING to CANCELED
2017-03-27 14:00:47 INFO  JobClient:145 - 03/27/2017 14:00:47	Custom Source -> Stream Sink(1/4) switched to CANCELED 
2017-03-27 14:00:47 DEBUG JobManager:86 - Handled message UpdateTaskExecutionState(TaskState jobId=849a403605bf5d7e8d14df90a927de42, executionId=867c029d7acfa61e26a52e01daa2acbb, state=CANCELED, error=(null)) in 0 ms from Actor[akka://flink/temp/$t].
2017-03-27 14:00:47 DEBUG SlotSharingGroupAssignment:492 - Release simple slot SimpleSlot (0)(0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:47 DEBUG Instance:293 - Return allocated slot Shared (0) - 7f747259085967c272ac26bfa10c5a15 @ 127.0.0.1 - 4 slots - URL: akka://flink/user/taskmanager_1 - RELEASED.
2017-03-27 14:00:47 DEBUG ExecutionGraph:667 - Socket Window WordCount switched from FAILING to FAILED.
2017-03-27 14:00:47 INFO  JobClient:145 - 03/27/2017 14:00:47	Job execution switched to status FAILED.
2017-03-27 14:00:47 DEBUG JobManager:86 - Received message 03/27/2017 14:00:47	Job execution switched to status FAILED. at akka://flink/user/jobmanager from Actor[akka://flink/deadLetters].
2017-03-27 14:00:47 INFO  JobManager:137 - Status of job 849a403605bf5d7e8d14df90a927de42 (Socket Window WordCount) changed to FAILED.
org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:47 DEBUG JobClient:118 - Received failure from JobManager
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:314)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
	at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:43)
	at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
	at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:92)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)
	at akka.dispatch.Mailbox.run(Mailbox.scala:221)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:231)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:397)
	at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:246)
	at org.apache.flink.api.java.io.TextOutputFormat.open(TextOutputFormat.java:77)
	at org.apache.flink.streaming.api.functions.sink.FileSinkFunction.open(FileSinkFunction.java:58)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:33)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:56)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openOperator(StreamTask.java:161)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.invoke(SourceStreamTask.java:52)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=20597, access=WRITE, inode="/user/zeas/stream/flink/2":zeas:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1780)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1764)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1747)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2558)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2493)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:227)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1389)
	... 20 more
2017-03-27 14:00:47 DEBUG JobManager:86 - Handled message 03/27/2017 14:00:47	Job execution switched to status FAILED. in 5 ms from Actor[akka://flink/deadLetters].
2017-03-27 14:00:47 DEBUG MemoryArchivist:86 - Received message ArchiveExecutionGraph(849a403605bf5d7e8d14df90a927de42,org.apache.flink.runtime.executiongraph.ExecutionGraph@6eaabe96) at akka://flink/user/archive from Actor[akka://flink/user/jobmanager#632585477].
2017-03-27 14:00:47 INFO  FlinkMiniCluster:148 - Stopping FlinkMiniCluster.
2017-03-27 14:00:47 DEBUG MemoryArchivist:86 - Handled message ArchiveExecutionGraph(849a403605bf5d7e8d14df90a927de42,org.apache.flink.runtime.executiongraph.ExecutionGraph@6eaabe96) in 0 ms from Actor[akka://flink/user/jobmanager#632585477].
2017-03-27 14:00:47 INFO  JobManager:128 - Stopping JobManager akka://flink/user/jobmanager#632585477.
2017-03-27 14:00:47 INFO  TaskManager:128 - Stopping TaskManager akka://flink/user/taskmanager_1#935127370.
2017-03-27 14:00:47 INFO  TaskManager:128 - Disassociating from JobManager
2017-03-27 14:00:47 DEBUG NetworkEnvironment:216 - Disassociating NetworkEnvironment from TaskManager. Cleaning all intermediate results.
2017-03-27 14:00:47 DEBUG NetworkEnvironment:221 - Shutting down network connection manager
2017-03-27 14:00:47 DEBUG NetworkEnvironment:233 - Shutting down intermediate result partition manager
2017-03-27 14:00:47 DEBUG JobManager:86 - Job manager akka://flink/user/jobmanager is completely stopped.
2017-03-27 14:00:47 DEBUG ResultPartitionManager:111 - Releasing 0 partitions because of shutdown.
2017-03-27 14:00:47 DEBUG ResultPartitionManager:122 - Successful shutdown.
2017-03-27 14:00:47 DEBUG IOManagerAsync:117 - Shutting down I/O manager.
2017-03-27 14:00:47 INFO  IOManager:127 - I/O manager removed spill file directory C:\Users\20597\AppData\Local\Temp\flink-io-595b6235-65bd-4161-8515-fc614d04f1e6
2017-03-27 14:00:47 DEBUG DefaultMemoryManager:163 - Shutting down MemoryManager instance org.apache.flink.runtime.memorymanager.DefaultMemoryManager@30c8d0aa
2017-03-27 14:00:47 INFO  TaskManager:128 - Task manager akka://flink/user/taskmanager_1 is completely shut down.
2017-04-03 16:39:40 INFO  ConsumerDriver:18 - 12 Arguments required, provided only 0 Arguments 
[]
